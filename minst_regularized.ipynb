{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.3536\n",
      "Epoch 101, Loss: 2.3393\n",
      "Epoch 201, Loss: 2.3132\n",
      "Epoch 301, Loss: 2.2594\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 87\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m reg_lambda \u001b[38;5;129;01min\u001b[39;00m lambdas:\n\u001b[0;32m     86\u001b[0m     W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m initialize_parameters(n_input, n_hidden, n_output)\n\u001b[1;32m---> 87\u001b[0m     W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg_lambda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predict(x_test, W1, b1, W2, b2)\n\u001b[0;32m     89\u001b[0m     test_labels \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39margmax(y_test, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 66\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(x_train, y_train, W1, b1, W2, b2, eta, epochs, reg_lambda)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     65\u001b[0m     z1, a1, z2, y_hat \u001b[38;5;241m=\u001b[39m forward_propagation(x_train, W1, b1, W2, b2)\n\u001b[1;32m---> 66\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcross_entropy_loss_with_regularization\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg_lambda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     dW1, db1, dW2, db2 \u001b[38;5;241m=\u001b[39m backward_propagation(x_train, y_train, W1, b1, W2, b2, a1, y_hat, reg_lambda)\n\u001b[0;32m     68\u001b[0m     W1 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m eta \u001b[38;5;241m*\u001b[39m dW1\n",
      "Cell \u001b[1;32mIn[14], line 18\u001b[0m, in \u001b[0;36mcross_entropy_loss_with_regularization\u001b[1;34m(predictions, targets, W1, W2, reg_lambda)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcross_entropy_loss_with_regularization\u001b[39m(predictions, targets, W1, W2, reg_lambda):\n\u001b[1;32m---> 18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     reg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m reg_lambda \u001b[38;5;241m*\u001b[39m (cp\u001b[38;5;241m.\u001b[39msum(W1\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m cp\u001b[38;5;241m.\u001b[39msum(W2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss \u001b[38;5;241m+\u001b[39m reg_loss\n",
      "Cell \u001b[1;32mIn[14], line 15\u001b[0m, in \u001b[0;36mcross_entropy_loss\u001b[1;34m(predictions, targets)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcross_entropy_loss\u001b[39m(predictions, targets):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m targets\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\cupy\\_math\\sumprod.py:40\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _fusion_thread_local\u001b[38;5;241m.\u001b[39mcall_reduction(\n\u001b[0;32m     37\u001b[0m         func, a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# TODO(okuta): check type\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "# Use numpy instead of cupy if you don't have a GPU\n",
    "def relu(x):\n",
    "    return cp.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return cp.where(x > 0, 1, 0)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = cp.exp(x - cp.max(x, axis=1, keepdims=True))\n",
    "    return e_x / cp.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(predictions, targets):\n",
    "    return -cp.sum(targets * cp.log(predictions + 1e-9)) / targets.shape[0]\n",
    "\n",
    "def cross_entropy_loss_with_regularization(predictions, targets, W1, W2, reg_lambda):\n",
    "    loss = cross_entropy_loss(predictions, targets)\n",
    "    reg_loss = 0.5 * reg_lambda * (cp.sum(W1**2) + cp.sum(W2**2))\n",
    "    return loss + reg_loss\n",
    "\n",
    "def predict(x, W1, b1, W2, b2):\n",
    "    z1 = cp.dot(x, W1.T) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = cp.dot(a1, W2.T) + b2\n",
    "    y_hat = softmax(z2)\n",
    "    return cp.argmax(y_hat, axis=1)\n",
    "\n",
    "def initialize_parameters(n_input=784, n_hidden=128, n_output=10):\n",
    "    cp.random.seed(42)\n",
    "    W1 = cp.random.randn(n_hidden, n_input) * 0.01\n",
    "    b1 = cp.zeros(n_hidden)\n",
    "    W2 = cp.random.randn(n_output, n_hidden) * 0.01\n",
    "    b2 = cp.zeros(n_output)\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def forward_propagation(x, W1, b1, W2, b2):\n",
    "    z1 = cp.dot(x, W1.T) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = cp.dot(a1, W2.T) + b2\n",
    "    y_hat = softmax(z2)\n",
    "    return z1, a1, z2, y_hat\n",
    "\n",
    "def backward_propagation(x, y, W1, b1, W2, b2, a1, y_hat, reg_lambda):\n",
    "    m = x.shape[0]\n",
    "    delta_2 = y_hat - y\n",
    "    dW2 = cp.dot(delta_2.T, a1) / m + reg_lambda * W2\n",
    "    db2 = cp.sum(delta_2, axis=0) / m\n",
    "    delta_1 = cp.dot(delta_2, W2) * relu_derivative(a1)\n",
    "    dW1 = cp.dot(delta_1.T, x) / m + reg_lambda * W1\n",
    "    db1 = cp.sum(delta_1, axis=0) / m\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def load_mnist():\n",
    "    from tensorflow.keras.datasets import mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = cp.asarray(x_train.reshape(-1, 784) / 255.0)\n",
    "    x_test = cp.asarray(x_test.reshape(-1, 784) / 255.0)\n",
    "    y_train = cp.eye(10)[y_train]\n",
    "    y_test = cp.eye(10)[y_test]\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def train(x_train, y_train, W1, b1, W2, b2, eta, epochs, reg_lambda):\n",
    "    for epoch in range(epochs + 1):\n",
    "        z1, a1, z2, y_hat = forward_propagation(x_train, W1, b1, W2, b2)\n",
    "        loss = cross_entropy_loss_with_regularization(y_hat, y_train, W1, W2, reg_lambda)\n",
    "        dW1, db1, dW2, db2 = backward_propagation(x_train, y_train, W1, b1, W2, b2, a1, y_hat, reg_lambda)\n",
    "        W1 -= eta * dW1\n",
    "        b1 -= eta * db1\n",
    "        W2 -= eta * dW2\n",
    "        b2 -= eta * db2\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x_train, y_train, x_test, y_test = load_mnist()\n",
    "    n_input, n_hidden, n_output = 784, 128, 10\n",
    "    eta = 0.01\n",
    "    epochs = 1000\n",
    "    lambdas = [0.01, 0.05, 0.1, 0.5, 1]\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "\n",
    "    for reg_lambda in lambdas:\n",
    "        W1, b1, W2, b2 = initialize_parameters(n_input, n_hidden, n_output)\n",
    "        W1, b1, W2, b2 = train(x_train, y_train, W1, b1, W2, b2, eta, epochs, reg_lambda)\n",
    "        predictions = predict(x_test, W1, b1, W2, b2)\n",
    "        test_labels = cp.argmax(y_test, axis=1)\n",
    "        accuracy = cp.mean(predictions == test_labels)\n",
    "        print(f\"Test Accuracy: {accuracy * 100:.2f}% with lambda = {reg_lambda}\")\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = (W1, b1, W2, b2)\n",
    "\n",
    "    print(f\"Best Test Accuracy: {best_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "x_train, y_train, x_test, y_test = load_mnist()\n",
    "# print(biggest_accuracy)\n",
    "fig, axes = plt.subplots(1, 10, figsize=(20, 2))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(x_train[i].reshape(28, 28).get(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Label: {np.argmax(y_train[i])}')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_mnist()\n",
    "\n",
    "fig, axes = plt.subplots(1, 8, figsize=(16, 2))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(x_test[i].reshape(28, 28).get(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Label: {np.argmax(y_test[i])}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cupy as cp\n",
    "\n",
    "W1, b1, W2, b2 = best_params\n",
    "test_predictions = predict(x_test, W1, b1, W2, b2)\n",
    "test_labels = cp.argmax(y_test, axis=1)  \n",
    "accuracy = cp.mean(test_predictions == test_labels)  \n",
    "print(f\"Accuracy: {100 * accuracy.get():.2f}%\")  \n",
    "\n",
    "fig, axes = plt.subplots(1, 8, figsize=(16, 2))\n",
    "for i, ax in enumerate(axes):\n",
    "   \n",
    "    ax.imshow(cp.asnumpy(x_test[i]).reshape(28, 28), cmap='gray')\n",
    "    ax.axis('off')\n",
    "   \n",
    "    ax.set_title(f'Pred: {int(test_predictions[i].get())}\\nTrue: {int(test_labels[i].get())}')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
